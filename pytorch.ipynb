{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FionaZZhang/DeepLearnMuse/blob/main/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQdNV6fDdHmr"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.transforms import ToTensor\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqdQlX3io1m6",
        "outputId": "88e9c00a-59ca-48d0-d370-31855d9702db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "ulIfCopJbiFn",
        "outputId": "1bd22c3c-badb-4d45-9070-ad35f9a7fe12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e74b9083a6f0>\u001b[0m in \u001b[0;36m<cell line: 551>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;31m# Forward pass through the model to get the class output and grad_cam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_cam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# Visualize the Grad-CAM on the original image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e74b9083a6f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_cam)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# Optionally: Compute Grad-CAM for ResNet part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_cam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             gradients = torch.autograd.grad(outputs=resnet_out, inputs=self.base_model[-2].output,\n\u001b[0m\u001b[1;32m    426\u001b[0m                                             grad_outputs=torch.ones(resnet_out.size()), create_graph=True)\n\u001b[1;32m    427\u001b[0m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'output'"
          ]
        }
      ],
      "source": [
        "# model settings\n",
        "MUSIC = \"/content/drive/MyDrive/ColabNotebooks/mel_spectrograms\"\n",
        "FEATURES = \"/content/drive/MyDrive/ColabNotebooks/features_30_sec.csv\"\n",
        "EPOCH = 100\n",
        "BATCH_SIZE = 64\n",
        "MODEL_NAME = \"testing\"\n",
        "PATIENCE = 10\n",
        "device = torch.device('cuda')\n",
        "\n",
        "\n",
        "# Data Loader and Model ------------------------------------------------------------------------------------------------\n",
        "# Data loader\n",
        "\n",
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "class resnet18(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(resnet18, self).__init__()\n",
        "\n",
        "        # Use a pre-trained ResNet model\n",
        "        self.base_model = models.resnet18(weights=True)\n",
        "        num_ftrs = self.base_model.fc.in_features\n",
        "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # remove last layer\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        # self.batchnorm = nn.BatchNorm1d(num_ftrs)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "        self._classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        # self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # x = self.leakyrelu(x)\n",
        "        x = self.dropout(x)\n",
        "        score = self._classifier(x)\n",
        "        return score\n",
        "\n",
        "    def _init_weights(self) -> None:\n",
        "        for layer in self.base_model.modules():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(layer.weight)\n",
        "            elif isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "class ResNetLSTM(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetLSTM, self).__init__()\n",
        "\n",
        "        # Use a pre-trained ResNet model\n",
        "        self.base_model = models.resnet18(pretrained=True)\n",
        "        num_ftrs = self.base_model.fc.in_features\n",
        "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # remove last layer\n",
        "\n",
        "        self.lstm = nn.LSTM(num_ftrs, 512, batch_first=True, bidirectional=True)  # LSTM layer\n",
        "        self._classifier = nn.Linear(512, num_classes)  # classifier layer based on LSTM output\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "\n",
        "        # The output from ResNet is a 3D tensor;\n",
        "        # We consider the two last dimensions as defining a sequence and apply LSTM\n",
        "        x = x.view(x.size(0), x.size(1), -1).transpose(1, 2)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]  # We take the output of the LSTM at the last timestep\n",
        "\n",
        "        score = self._classifier(x)\n",
        "        return score\n",
        "\n",
        "    def _init_weights(self) -> None:\n",
        "        for layer in self.base_model.modules():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(layer.weight)\n",
        "            elif isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "\n",
        "class ResNetGRU(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetGRU, self).__init__()\n",
        "\n",
        "        # Use a pre-trained ResNet model\n",
        "        self.base_model = models.resnet18(pretrained=True)\n",
        "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # remove last layer\n",
        "\n",
        "        # Calculate the correct number of features\n",
        "        dummy_input = torch.zeros(1, 3, 224, 224)\n",
        "        dummy_output = self.base_model(dummy_input)\n",
        "        num_ftrs = dummy_output.size(1) * dummy_output.size(2)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(in_channels=num_ftrs, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.rnn = nn.GRU(input_size=64, hidden_size=100, num_layers=3, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(200, num_classes)\n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=100)  # Add BatchNorm layer\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "        x = self.base_model(x)\n",
        "        x = x.view(x.size(0), -1, x.size(3))\n",
        "        x = self.conv1d(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x, _ = self.rnn(x)\n",
        "        x_forward = x[:, -1, :self.rnn.hidden_size]\n",
        "        x_backward = x[:, 0, self.rnn.hidden_size:]\n",
        "        x = torch.cat((x_forward, x_backward), dim=1)\n",
        "        x = self.dropout(x)\n",
        "        # x = self.batch_norm(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        # GRU part for sequence-like processing\n",
        "        self.conv1d = nn.Conv1d(in_channels=224, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.rnn = nn.GRU(input_size=672, hidden_size=256, num_layers=3, batch_first=True, bidirectional=True)\n",
        "        self.gru_fc = nn.Linear(512, 256)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "\n",
        "        x = x.permute(0, 1, 3, 2)\n",
        "\n",
        "        x_gru = x.contiguous().view(x.size(0), x.size(2), -1)\n",
        "        x_gru = self.conv1d(x_gru)\n",
        "        x_gru, _ = self.rnn(x_gru)\n",
        "        x_gru_forward = x_gru[:, -1, :self.rnn.hidden_size]\n",
        "        x_gru_backward = x_gru[:, 0, self.rnn.hidden_size:]\n",
        "        x_gru = torch.cat((x_gru_forward, x_gru_backward), dim=1)\n",
        "        gru_out = self.gru_fc(x_gru)\n",
        "\n",
        "        out = self.dropout(gru_out)\n",
        "        result = self.fc(out)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention = nn.Linear(self.hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, output):\n",
        "        # output => [batch_size, seq_len, hidden_size*2]\n",
        "        energy = self.attention(output)\n",
        "        attention = F.softmax(energy.squeeze(-1), dim=1)\n",
        "        # attention => [batch_size, seq_len]\n",
        "        return attention.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class ParallelResNetGRUAttention(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ParallelResNetGRUAttention, self).__init__()\n",
        "\n",
        "        # ResNet part for image-like processing\n",
        "        self.base_model = models.resnet18(pretrained=True)\n",
        "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.resnet_fc = nn.Linear(512, 256)\n",
        "\n",
        "        # GRU part for sequence-like processing\n",
        "        self.conv1d = nn.Conv1d(in_channels=224, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.rnn = nn.GRU(input_size=672, hidden_size=256, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.attention = Attention(hidden_size=256)  # Add attention layer\n",
        "        self.gru_fc = nn.Linear(512, 256)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=512)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "\n",
        "        # ResNet forward pass\n",
        "        resnet_out = self.base_model(x)\n",
        "        resnet_out = self.adaptive_pool(resnet_out)\n",
        "        resnet_out = torch.flatten(resnet_out, 1)\n",
        "        resnet_out = self.resnet_fc(resnet_out)\n",
        "\n",
        "        # GRU forward pass\n",
        "        x_gru = x.view(x.size(0), x.size(2), -1)\n",
        "        x_gru = self.conv1d(x_gru)\n",
        "        x_gru, _ = self.rnn(x_gru)\n",
        "\n",
        "        # Apply attention\n",
        "        attention_weights = self.attention(x_gru)\n",
        "        x_gru = (x_gru * attention_weights).sum(dim=1)\n",
        "\n",
        "        gru_out = self.gru_fc(x_gru)\n",
        "\n",
        "        # Combine ResNet and GRU outputs\n",
        "        out = torch.cat((resnet_out, gru_out), dim=1)\n",
        "        out = self.dropout(out)\n",
        "        result = self.fc(out)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(401408, 1024)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        score = self.fc2(x)\n",
        "        return score\n",
        "\n",
        "\n",
        "class FcnCnn(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FcnCnn, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.25),\n",
        "        )\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)\n",
        "        dummy_output = self.conv_layers(dummy_input)\n",
        "        self.output_size = int(np.prod(dummy_output.shape))\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(self.output_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten the tensor\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=False, delta=-0.03):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "class ParallelResNetGRU(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ParallelResNetGRU, self).__init__()\n",
        "\n",
        "        # ResNet part for image-like processing\n",
        "        self.base_model = models.resnet18(pretrained=True)\n",
        "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])\n",
        "        self.adaptive_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "        self.resnet_fc = nn.Linear(512, 256)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
        "\n",
        "        # GRU part for sequence-like processing\n",
        "        self.rnn = nn.GRU(input_size=672, hidden_size=256, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.gru_fc = nn.Linear(512, 256)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=512)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        self._init_weight(self.fc)\n",
        "\n",
        "    def _init_weight(self, layer):\n",
        "        if isinstance(layer, nn.Linear) or isinstance(layer, nn.Conv1d):\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "\n",
        "        # ResNet forward pass\n",
        "        resnet_out = self.base_model(x)\n",
        "        resnet_out = self.adaptive_pool(resnet_out)\n",
        "        resnet_out = torch.flatten(resnet_out, 1)\n",
        "        resnet_out = self.resnet_fc(resnet_out)\n",
        "        resnet_out = self.dropout(resnet_out)\n",
        "        resnet_out = self.batch_norm1(resnet_out)\n",
        "\n",
        "        # GRU forward pass\n",
        "        x = x.permute(0, 1, 3, 2)\n",
        "        x_gru = x.contiguous().view(x.size(0), x.size(2), -1)\n",
        "        x_gru, _ = self.rnn(x_gru)\n",
        "        x_gru_forward = x_gru[:, -1, :self.rnn.hidden_size]\n",
        "        x_gru_backward = x_gru[:, 0, self.rnn.hidden_size:]\n",
        "        x_gru = torch.cat((x_gru_forward, x_gru_backward), dim=1)\n",
        "        gru_out = self.gru_fc(x_gru)\n",
        "\n",
        "        # Combine ResNet and GRU outputs\n",
        "        out = torch.cat((resnet_out, gru_out), dim=1)\n",
        "        out = self.dropout(out)\n",
        "        result = self.fc(out)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class ParallelResNetGRUSimp(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ParallelResNetGRUSimp, self).__init__()\n",
        "\n",
        "        # ResNet part for image-like processing\n",
        "        self.base_model = models.resnet18(pretrained=True)\n",
        "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])\n",
        "        self.adaptive_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "        self.resnet_fc = nn.Linear(512, 100)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(100)\n",
        "\n",
        "        # GRU part for sequence-like processing\n",
        "        self.rgb_to_grayscale = transforms.Grayscale(num_output_channels=1)\n",
        "        self.rnn = nn.GRU(input_size=224, hidden_size=50, num_layers=1, batch_first=True, bidirectional = True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(100)\n",
        "\n",
        "        self.fc = nn.Linear(200, num_classes)\n",
        "\n",
        "    #     self._init_weight(self.fc)\n",
        "\n",
        "    # def _init_weight(self, layer):\n",
        "    #     if isinstance(layer, nn.Linear) or isinstance(layer, nn.Conv1d):\n",
        "    #         nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "\n",
        "        # ResNet forward pass\n",
        "        resnet_out = self.base_model(x)\n",
        "        resnet_out = self.adaptive_pool(resnet_out)\n",
        "        resnet_out = torch.flatten(resnet_out, 1)\n",
        "        resnet_out = self.resnet_fc(resnet_out)\n",
        "        resnet_out = self.dropout(resnet_out)\n",
        "        resnet_out = self.batch_norm1(resnet_out)\n",
        "\n",
        "\n",
        "        # GRU forward pass\n",
        "        x = self.rgb_to_grayscale(x)\n",
        "        x = x.permute(0, 1, 3, 2)\n",
        "        x_gru = x.contiguous().view(x.size(0), x.size(2), -1)\n",
        "        x_gru, _ = self.rnn(x_gru)\n",
        "        x_gru_forward = x_gru[:, -1, :self.rnn.hidden_size]\n",
        "        x_gru_backward = x_gru[:, 0, self.rnn.hidden_size:]\n",
        "        x_gru = torch.cat((x_gru_forward, x_gru_backward), dim=1)\n",
        "        # x_gru = self.gru_fc(x_gru)\n",
        "        # x_gru = self.batch_norm2(x_gru)\n",
        "\n",
        "        # Combine ResNet and GRU outputs\n",
        "        out = torch.cat((resnet_out, x_gru), dim=1)\n",
        "        # out = self.dropout(out)\n",
        "        result = self.fc(out)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "# Here it starts ------------------------------------------------------------------------------------------------------\n",
        "# Get music features\n",
        "features_df = pd.read_csv(FEATURES)\n",
        "# Encoded genres\n",
        "genres = { 'rock': 0, 'pop': 1, 'classical': 2, 'reggae': 3, 'disco': 4, 'jazz': 5, 'metal': 6, 'country': 7, 'blues': 8, 'hiphop': 9}\n",
        "\n",
        "# get music images\n",
        "music_dataset = []\n",
        "genre_target = []\n",
        "song_ids = []\n",
        "for name in os.listdir(MUSIC):\n",
        "  filename = os.path.join(MUSIC, name)\n",
        "  music_dataset.append(filename)\n",
        "  genre, song_id, png = name.split(\".\")\n",
        "  genre_target.append(genre)\n",
        "  song_ids.append(genre + str(int(song_id) // 5))\n",
        "\n",
        "# Convert the genre labels to a tensor\n",
        "genre_encoded = [genres[item] for item in genre_target]\n",
        "genre_encoded_tensor = torch.tensor(genre_encoded, dtype=torch.long)\n",
        "\n",
        "# Create a MusicDataset object to load the images, features, and labels\n",
        "music_dataset = MusicDataset(music_dataset, genre_encoded_tensor, transform)\n",
        "music_dataloader = DataLoader(music_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Now, use the song IDs to perform a group-based train/test split\n",
        "group_shuffle_split = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "# We use the splitter to split the indices of our dataset\n",
        "train_inds, test_inds = next(group_shuffle_split.split(music_dataset, genre_encoded, groups=song_ids))\n",
        "\n",
        "# We use these indices to produce the final train and test sets\n",
        "train_dataset = Subset(music_dataset, train_inds)\n",
        "test_dataset = Subset(music_dataset, test_inds)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize the model\n",
        "model = ParallelResNetGRU(num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "early_stopping = EarlyStopping(patience=PATIENCE, verbose=True)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "num_epochs = EPOCH\n",
        "actual_epochs = 0\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0.0\n",
        "    train_total = 0.0\n",
        "    actual_epochs += 1\n",
        "\n",
        "    for i, (image, labels) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * image.size(0)  # scale by batch size\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(train_dataloader.dataset)\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0.0\n",
        "    test_total = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, labels in test_dataloader:\n",
        "            outputs = model(image.to(device))\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "            test_loss += loss.item() * image.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels.to(device)).sum().item()\n",
        "            # collect predictions and true labels\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss / len(test_dataloader.dataset)\n",
        "    test_acc = test_correct / test_total\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    train_loss_list.append(train_loss)\n",
        "    test_loss_list.append(test_loss)\n",
        "    y_true_list.extend(y_true)\n",
        "    y_pred_list.extend(y_pred)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "    # Early stopping\n",
        "    early_stopping(test_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "num_epochs = actual_epochs\n",
        "model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ColabNotebooks/results/model_' + MODEL_NAME + '_best.pt')\n",
        "\n",
        "# Plot the training and testing accuracy over time\n",
        "plt.plot(range(num_epochs), train_acc_list, label='Training Accuracy')\n",
        "plt.plot(range(num_epochs), test_acc_list, label='Testing Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/MyDrive/ColabNotebooks/results/model_' + MODEL_NAME + '_acc.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot the training and testing loss over time\n",
        "plt.plot(range(num_epochs), train_loss_list, label='Training Loss')\n",
        "plt.plot(range(num_epochs), test_loss_list, label='Testing Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/MyDrive/ColabNotebooks/results/model_' + MODEL_NAME + '_loss.png')\n",
        "plt.close()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_list, y_pred_list))\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_true_list, y_pred_list)\n",
        "\n",
        "# Plot confusion matrix\n",
        "label_names = ['rock', 'pop', 'classical', 'reggae', 'disco', 'jazz', 'metal', 'country', 'blues', 'hiphop']\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "ax.set(xticks=np.arange(cm.shape[1]),\n",
        "       yticks=np.arange(cm.shape[0]),\n",
        "       xticklabels=label_names, yticklabels=label_names,\n",
        "       title='Confusion matrix',\n",
        "       ylabel='True label',\n",
        "       xlabel='Predicted label')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
        "fig.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/ColabNotebooks/results/CM_model_' + MODEL_NAME + '.png')\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}